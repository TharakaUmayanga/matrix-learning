Activation functions and error functions are important components  in machine learning. Activation functions introduce non-linearity into neural networks, while error functions measure how well a model is performing on a given task. The choice of activation and error function depends on the specific problem being solved and the characteristics of the data.

# Activation Functions

In machine learning, activation functions are used to introduce non linearity into neural networks, which is necessary for the network to learn complex patterns and relationships in data. An activation function takes the weighted sum of the input data and bias term as input and produces an output value that is fed into the next layer of the neural network.

 - [x] Sigmoid Function
- [x] Rectified Linear Unit (ReLU)
- [x] Leaky Relu
- [ ] Softmax Function
- [ ] Hyperbolic Tangent Function (tanh)
- [ ] Exponential Linear Units (ELU)

# Error Functions

In machine learning, an error function (also known as a loss function or cost function) is a measure of how well a model is performing on a given task. The error function is typically defined mathematically and is used to evaluate the difference between the predicted output of the model and the actual output.

- [ ] Mean Squared Error (MSE)
- [ ] Mean Absolute Error (MAE)
- [ ] Binary Cross-Entropy Loss
- [ ] Categorical Cross-Entropy Loss
- [ ] Hinge Loss
- [ ] Huber Loss
- [ ] Kullback-Leibler (KL) Divergence
- [ ] Mean Squared Logarithmic Error (MSLE)



## Sigmoid Function

The sigmoid function is a mathematical function that is often used in machine learning and artificial neural networks. It is a type of activation function that maps any real-valued number to a value between 0 and 1, which makes it particularly useful for tasks that involve binary classification.

$$ \alpha (x) =  \frac{\mathrm{1} }{\mathrm{1} + e^-x }  $$ 


